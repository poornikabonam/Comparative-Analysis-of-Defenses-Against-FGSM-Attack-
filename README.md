# Comparative-Analysis-of-Defenses-Against-FGSM-Attack
## Overview

This repository investigates the vulnerabilities of a Deep Neural Network (DNN) model trained on the MNIST dataset, emphasizing the impact of adversarial attacks and evaluating various defense mechanisms.

## Environment

- **Programming Language:** Python
- **Libraries:** TensorFlow, NumPy, Matplotlib
- **Framework:** Keras
- **Dataset:** MNIST

## Methodology

1. **Model Architecture:**
   - The DNN model architecture is based on [insert architecture details].

2. **Training:**
   - The model is trained on the MNIST dataset using [insert training details].

3. **Adversarial Attacks:**
   - Implementations of adversarial attacks, such as the Fast Gradient Sign Method (FGSM), are included.

4. **Defense Mechanisms:**
   - Explore the effectiveness of the following defense mechanisms:
     - Adversarial Training
     - Adversarial Training with Gradient Masking
     - Label Smoothing
     - Distilled Neural Network

5. **Results Analysis:**
   - Visualize and analyze the impact of adversarial attacks on the model.
   - Evaluate the performance of each defense mechanism in mitigating adversarial perturbations.

## Results

### Adversarial Attack Impact

[Include visualizations and analysis of the model's performance under adversarial attacks.]

### Defense Mechanism Evaluation

[Present results and comparisons of different defense mechanisms.]

## Usage
